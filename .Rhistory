} else if (dist == 2) { ## mutual friends
currentValue <- userMap[[user]]
if (!is.null(currentValue)) {
if (!is.na(currentValue)) { ## bump the count
userMap[[user]] <- currentValue + 1
}
} else {  ## count 1
userMap[[user]] <- 1
}
}
}
## Drop direct friends
for (user in keys(userMap)) {
if (is.na(userMap[[user]])) {
del(user, userMap)
}
}
list <- as.list(userMap)
order <- order(as.integer(list), decreasing=TRUE)
list <- list[order]
storage.mode(list) <- "integer"
keyMap[[key]] <- list
}
#sink("recommend-friends.txt")
catn("Recommendation, top row is user, bottom row frequency")
#sink("recommend-friends.txt")
cat("Recommendation, top row is user, bottom row frequency")
for (key in keys(keyMap)) {
catn("Recommendation for User", key)
print(keyMap[[key]])
}
install.packages("rapportools")
library(rapportools)
catn("Recommendation, top row is user, bottom row frequency")
for (key in keys(keyMap)) {
catn("Recommendation for User", key)
print(keyMap[[key]])
}
lines <- readLines("soc-LiveJournal1Adj.txt")
records <- lapply(lines, function(x) as.integer(strsplit(x, split="[\t,]")[[1]]))
records
library(glmnet)
require(doSNOW)
library(doSNOW)
install.packages(doSNOW)
install.packages("doSNOW")
## SNOW Run
library(glmnet)
library(doSNOW)
runSnow <- function(n, seed = 12345) {
catn <- function(...) cat(..., "\n")
internetAd <- readRDS("internetAd.RDS")
catn("Percent non-zero per glmnet paper",
(sum(internetAd$x > 0) + sum(internetAd$y > 0)) /
(prod(dim(internetAd$x)) + length(internetAd$y)))
stopifnot(require(doSNOW))
cl <- makeCluster(n)
registerDoSNOW(cl)
set.seed(seed)
time <- system.time(cv <- cv.glmnet(internetAd$x, internetAd$y,
family = "binomial", type.measure = "class",
parallel = TRUE))
stopCluster(cl)
list(time = time, cv = cv)
}
resultsSnow <- lapply(2:4, runSnow)
runSnow <- function(n, seed = 12345) {
catn <- function(...) cat(..., "\n")
internetAd <- readRDS("internetAd.RDS")
catn("Percent non-zero per glmnet paper",
(sum(internetAd$x > 0) + sum(internetAd$y > 0)) /
(prod(dim(internetAd$x)) + length(internetAd$y)))
stopifnot(require(doSNOW))
cl <- makeCluster(n)
registerDoSNOW(cl)
set.seed(seed)
time <- system.time(cv <- cv.glmnet(internetAd$x, internetAd$y,
family = "binomial", type.measure = "class",
parallel = TRUE))
stopCluster(cl)
list(time = time, cv = cv)
}
resultsSnow <- lapply(2:5, runSnow)
library(ggplot2)
d <- data.frame(nWorkers = 2:4, t(sapply(resultsSnow, function(x) x$time)))
resultsSnow <- lapply(2:4, runSnow)
library(ggplot2)
d <- data.frame(nWorkers = 2:4, t(sapply(resultsSnow, function(x) x$time)))
qplot(x = nWorkers, y = user.self, geom = "line", data = d)
lapply(resultsSnow, function(x) x$cv$lambda.min)
install.packages("doParallel")
library(doParallel)
runParallel <- function(n, seed = 12345) {
catn <- function(...) cat(..., "\n")
internetAd <- readRDS("internetAd.RDS")
catn("Percent non-zero per glmnet paper",
(sum(internetAd$x > 0) + sum(internetAd$y > 0)) /
(prod(dim(internetAd$x)) + length(internetAd$y)))
stopifnot(require(doParallel))
registerDoParallel(n)
set.seed(seed)
time <- system.time(cv <- cv.glmnet(internetAd$x, internetAd$y,
family = "binomial", type.measure = "class",
parallel = TRUE))
list(time = time, cv = cv)
}
resultsParallel <- lapply(2:4, runParallel)
d <- data.frame(nWorkers = 2:4, t(sapply(resultsParallel, function(x) x$time)))
qplot(x = nWorkers, y = user.self, geom="line", data = d)
lapply(resultsParallel, function(x) x$cv$lambda.min)
snowPlot <- function(n, seed = 12345) {
catn <- function(...) cat(..., "\n")
internetAd <- readRDS("internetAd.RDS")
catn("Percent non-zero per glmnet paper",
(sum(internetAd$x > 0) + sum(internetAd$y > 0)) /
(prod(dim(internetAd$x)) + length(internetAd$y)))
stopifnot(require(doSNOW))
cl <- makeCluster(n)
registerDoSNOW(cl)
set.seed(seed)
plot <- snow.time(cv <- cv.glmnet(internetAd$x, internetAd$y,
family = "binomial", type.measure = "class",
parallel = TRUE))
stopCluster(cl)
list(plot = plot, cv = cv)
}
snowResults <- snowPlot(n = 3)
plot(snowResults$plot)
setwd("~/GitHub/stats290")
user_graph <- tweets_user %>%
select(user, word, count) %>%
filter(count > 20)
library(magrittr)
user_graph <- tweets_user %>%
select(user, word, count) %>%
filter(count > 20)
tweets_user <- tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user, word) %>%
summarise(count = n()) %>%
left_join(tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user) %>%
summarise(total = n())) %>%
mutate(freq = count/total)
library(dplyr) ## as_tibble
library(magrittr) ## %>%
library(datetime) ## as.time
library(tidytext) ##str_detect str_remove_all unnest_tokens stop_words
## library(lubridate)
sentiment.orig <- tibble()
sentiment <- tibble()
sentiment.words <- tibble()
readData <- function(file = "Sentiment140.csv") {
sentiment.orig <<- as_tibble(read_csv(file))
## head(sentiment.orig)
names(sentiment.orig) <- c("polarity", "id", "date_orig", "query", "user", "tweet")
## names(sentiment.orig)
## extract date, time, hr
sentiment <<- sentiment.orig %>% mutate(
date = as.Date(paste(substr(sentiment.orig$date_orig,5,10),substr(sentiment.orig$date_orig,25,29)), format = "%B %d %Y"),
time = as.time(substr(sentiment.orig$date_orig, 12,19)),
hr = substr(sentiment.orig$date_orig,12,13)
) %>% mutate (
weekday = weekdays(date)
)
## words
remove_reg <- "&amp;|&lt;|&gt;"
sentiment.words <<- sent %>%
filter(!str_detect(tweet, "^RT")) %>%
mutate(tweet = str_remove_all(tweet, remove_reg)) %>%
unnest_tokens(word, tweet, token = "tweets") %>%
filter(!word %in% stop_words$word,
!word %in% str_remove_all(stop_words$word, "'"),
str_detect(word, "[a-z]"))
##saveRDS(sentiment.orig, file = "sentiment-orig.rds")
##saveRDS(sentiment, file = "sentiment.rds")
##saveRDS(sentiment.words, file = "sentiment-words.rds")
}
readData("sentiment")
tidy_tweets <- readRDS("sentiment.rds")
tidy_tweets
tidy_tweets <- readRDS("sentiment-word.rds")
tidy_tweets
tidy_tweets$word
tidy_tweets <- readRDS("sentiment-word.rds")
tidy_tweets <- readRDS("sentiment-words.rds")
tidy_tweets$word
tweets_user <- tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user, word) %>%
summarise(count = n()) %>%
left_join(tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user) %>%
summarise(total = n())) %>%
mutate(freq = count/total)
library(tidyverse)
tweets_user <- tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user, word) %>%
summarise(count = n()) %>%
left_join(tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user) %>%
summarise(total = n())) %>%
mutate(freq = count/total)
tweets_user
tweets_user %>%
select(user, word, count) %>%
filter(count > 20)
tweets_user %>%
select(user, word, count) %>%
filter(count > 25)
user_graph <- tweets_user %>%
select(user, word, count) %>%
filter(count > 25) %>%
graph_from_data_frame()
library(igraph)
user_graph <- tweets_user %>%
select(user, word, count) %>%
filter(count > 25) %>%
graph_from_data_frame()
user_graph
library(ggraph) ##ggraph
set.seed(2019)
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
tweets_user %>%
select(user, word, count, total) %>%
##filter(count > 25) %>%
filter(total > 50)
tweets_user %>%
select(user, word, count, total) %>%
filter(total > 100) %>%
filter(count > 25)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
filter(total > 100) %>%
filter(count > 25) %>%
graph_from_data_frame()
user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
filter(total > 100) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
filter(total > 200) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, str_replace_all(word, "@",""), count, total) %>%
filter(total > 200) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
tweets_user %>%
select(user, str_replace_all(word, "@",""), count, total)
str_remove("@something","@")
tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@"))
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > 150) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > 100) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
##filter(total > 100) %>%
filter(count > 20) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > 100) %>%
filter(count > 20) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > 100) %>%
filter(count > 10) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
userInterractions <- function(all_tweets = 100, user_mentions = 10) {
tidy_tweets <- readRDS("sentiment-words.rds")
tweets_user <- tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user, word) %>%
summarise(count = n()) %>%
left_join(tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user) %>%
summarise(total = n())) ##%>%    mutate(freq = count/total)
set.seed(2019)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > all_tweets) %>%
filter(count > user_mentions) %>%
graph_from_data_frame()
## user_graph
ggraph(user_graph, layout = "fr") +
geom_edge_link() +
geom_node_point() +
geom_node_text(aes(label = name), vjust = 1, hjust = 1)
}
userInterractions(120, 12)
ggraph(user_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = count), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(user_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = count), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
userInterractions <- function(all_tweets = 100, user_mentions = 10) {
tidy_tweets <- readRDS("sentiment-words.rds")
tweets_user <- tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user, word) %>%
summarise(count = n()) %>%
left_join(tidy_tweets %>%
filter(str_detect(word, "^@")) %>%
group_by(user) %>%
summarise(total = n())) ##%>%    mutate(freq = count/total)
set.seed(2019)
user_graph <- tweets_user %>%
select(user, word, count, total) %>%
mutate(word = str_remove(word, "@")) %>%
filter(total > all_tweets) %>%
filter(count > user_mentions) %>%
graph_from_data_frame()
## user_graph
##  ggraph(user_graph, layout = "fr") +
##    geom_edge_link() +
##    geom_node_point() +
##    geom_node_text(aes(label = name), vjust = 1, hjust = 1)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(user_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = count), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 5) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
}
userInterractions(100, 15)
userInterractions(150, 10)
sent <- readRDS("sentiment.rds")
min(sent$date)
max(sent$date)
mindate <- min(sent$date)
maxdate <- max(sent$date)
library(sqldf) ##sqldf
library(gridExtra) ##grid.arrange
library(ggplot) ##ggplot qplot
describeTweets <- function() {
sent <- readRDS("sentiment.rds")
sent.words <- readRDS("sentiment-words.rds")
by_weeekday <- sent %>% group_by(weekday) %>% summarise(count = n()) %>% arrange(count)
by_hour <- sent %>% group_by(hr) %>% summarise(count = n()) %>% arrange(hr)
by_user <- sent %>% group_by(user) %>% summarise(count = n()) %>% arrange(desc(count))
by_word <- sent.words %>% group_by(word) %>% summarise(count = n()) %>% arrange(desc(count))
ntweets <- nrow(sent)
nusers <- length(unique(sent$user))
mindate <- min(sent$date)
maxdate <- max(sent$date)
## plot tweets by day of the week
plot1 <- ggplot(by_weeekday,
aes(x = reorder(weekday, count), count)) +
geom_bar(stat = "identity", fill = "lightblue4") +
xlab("Count") +
ylab("Weekday") +
labs(title="Tweets by Day of the Week")+
##coord_flip() +
theme_classic()
## plot tweets by hour of the day
plot2 <- ggplot(by_hour,
aes(x = hr, count)) +
geom_bar(stat = "identity", fill = "skyblue3") +
xlab("Count") +
ylab("Hour") +
labs(title="Tweets by Hour of the Day")+
##coord_flip() +
theme_classic()
## list top 10 users by number of tweets
plot3 <- qplot(1:10, 1:10, geom = "blank") + theme_classic() + theme(line = element_blank(), text = element_blank()) +
annotation_custom(grob = tableGrob(as.data.frame(head(by_user,10)))) ## + labs(title="Top 10 Users")
## list top 10 most frequent words used
plot4 <- qplot(1:10, 1:10, geom = "blank") + theme_classic() + theme(line = element_blank(), text = element_blank()) +
annotation_custom(grob = tableGrob(as.data.frame(head(by_word,10)))) ## + labs(title="Top 10 Users")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, top = paste("Total number of tweets: ", format(ntweets,big.mark=",",scientific=FALSE),
", Total number of users: " , format(nusers,big.mark=",",scientific=FALSE),
", Date range: ", mindate, "-", maxdate))
}
describeTweets()
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, top = paste("Total number of tweets: ", format(ntweets,big.mark=",",scientific=FALSE),
", Total number of users: " , format(nusers,big.mark=",",scientific=FALSE),
", Date range: ", mindate, "-", maxdate, "\n"))
describeTweets <- function() {
sent <- readRDS("sentiment.rds")
sent.words <- readRDS("sentiment-words.rds")
by_weeekday <- sent %>% group_by(weekday) %>% summarise(count = n()) %>% arrange(count)
by_hour <- sent %>% group_by(hr) %>% summarise(count = n()) %>% arrange(hr)
by_user <- sent %>% group_by(user) %>% summarise(count = n()) %>% arrange(desc(count))
by_word <- sent.words %>% group_by(word) %>% summarise(count = n()) %>% arrange(desc(count))
ntweets <- nrow(sent)
nusers <- length(unique(sent$user))
mindate <- min(sent$date)
maxdate <- max(sent$date)
## plot tweets by day of the week
plot1 <- ggplot(by_weeekday,
aes(x = reorder(weekday, count), count)) +
geom_bar(stat = "identity", fill = "lightblue4") +
xlab("Count") +
ylab("Weekday") +
labs(title="Tweets by Day of the Week")+
##coord_flip() +
theme_classic()
## plot tweets by hour of the day
plot2 <- ggplot(by_hour,
aes(x = hr, count)) +
geom_bar(stat = "identity", fill = "skyblue3") +
xlab("Count") +
ylab("Hour") +
labs(title="Tweets by Hour of the Day")+
##coord_flip() +
theme_classic()
## list top 10 users by number of tweets
plot3 <- qplot(1:10, 1:10, geom = "blank") + theme_classic() + theme(line = element_blank(), text = element_blank()) +
annotation_custom(grob = tableGrob(as.data.frame(head(by_user,10)))) ## + labs(title="Top 10 Users")
## list top 10 most frequent words used
plot4 <- qplot(1:10, 1:10, geom = "blank") + theme_classic() + theme(line = element_blank(), text = element_blank()) +
annotation_custom(grob = tableGrob(as.data.frame(head(by_word,10)))) ## + labs(title="Top 10 Users")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, top = paste("Total number of tweets: ", format(ntweets,big.mark=",",scientific=FALSE),
", Total number of users: " , format(nusers,big.mark=",",scientific=FALSE),
", Date range: ", mindate, "-", maxdate, "\n"))
}
describeTweets()
install.packages("NLP", "openNLP")
installed.packages()
library(NLP)
library(openNLP)
install.packages("openNLP")
library(openNLP)
sentence.annot = Maxent_Sent_Token_Annotator()
word.annot = Maxent_Word_Token_Annotator()
?Maxent_Word_Token_Annotator()
?Maxent_Sent_Token_Annotator
library(NLP)
library(openNLP)
library(openNLP)
